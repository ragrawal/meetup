{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: Top 10 Most Rated Movie\n",
    "\n",
    "This is a two step process. First we use u.data to determine how often particular movie id is rated. Second we use output of the first step and join it to u.item dataset in order to get movie titles. Our final output will contain following three fields:\n",
    "1. Movie ID\n",
    "2. Movie Title\n",
    "3. Number of times it was rated\n",
    "\n",
    "Below is some information about data\n",
    "1. u.data\n",
    "    * This file contains actual movie rating data. \n",
    "    * Field separator: tab\n",
    "    * Fields: user id, movie id, rating (from 1 to 5) and timestamp\n",
    "2. u.item\n",
    "    * this file contains information about movies.\n",
    "    * Field separator: | (pipe)\n",
    "    * Fields: movie id | movie title | release date | video release date | IMDb URL | unknown | Action | Adventure | Animation | Children's | Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dataStrToTuple(inString, delimiter=\"\\t\"):\n",
    "    \"\"\"\n",
    "    Splits inString using delimiter and returns a tuple of integers. \n",
    "    It assumes that all the input tokens are integer\n",
    "    \"\"\"\n",
    "    tokens = [int(y) for y in inString.split(delimiter)]\n",
    "    return tuple(tokens)\n",
    "\n",
    "# load dataset\n",
    "data = sc.textFile(\"data/meetup/movielens/u.data\", 5)\n",
    "\n",
    "# convert input record to tuple\n",
    "rdd1 = data.map(dataStrToTuple)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# extract movie id as the first element so that we can use it as a key. \n",
    "# Value can be left null/empty as in the next stage we only need to count number of records per key\n",
    "rdd2 = rdd1.map(lambda x: (x[1],1)) \n",
    "# print rdd2.take(5)\n",
    "# type(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group by movie id and count number of elements\n",
    "cnts = rdd2.countByKey()\n",
    "# print \"\".join([\"{0},{1}\\n\".format(k, rdd3[k]) for k in rdd3.keys()[0:5]])\n",
    "# type(cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 583)\n",
      "(258, 509)\n",
      "(100, 508)\n",
      "(181, 507)\n",
      "(294, 485)\n",
      "(286, 481)\n",
      "(288, 478)\n",
      "(1, 452)\n",
      "(300, 431)\n",
      "(121, 429)\n"
     ]
    }
   ],
   "source": [
    "# countByKey returns a collection object \n",
    "for row in sorted(cnts.items(), key=lambda x: x[1], reverse=True)[0:10]:\n",
    "    print row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Challenge: Distributed Sorting\n",
    "countByKey returned a collection object that we sorted in memory. How would you modify the above program so that we use spark's distributed computing engine to extract top 10 most rated movie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 583), (258, 509), (100, 508), (181, 507), (294, 485), (286, 481), (288, 478), (1, 452), (300, 431), (121, 429)]\n"
     ]
    }
   ],
   "source": [
    "rdd4 = rdd2.reduceByKey(lambda x, y: x + y)\n",
    "print rdd4.takeOrdered(10, key=lambda x: -1 * x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Challenge: Multikey Sorting\n",
    "Above takeOrdered uses only value (i.e. number of times a movie was rated) to extract top 10 movies. However if there are lot of movies that were rated by the same number of people then its possible that different analyst will return different list for top 10 movies. Can you modify the above code to sort movies both by value and movie id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(50, 583), (258, 509), (100, 508), (181, 507), (294, 485), (286, 481), (288, 478), (1, 452), (300, 431), (121, 429)]\n"
     ]
    }
   ],
   "source": [
    "print rdd4.takeOrdered(10, key=lambda x: (-1 * x[1], x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Add Movie Title \n",
    "To make sense of the top 10 most rated movie now lets add movie title. Movie information is stored in u.item file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, u'Toy Story (1995)'), (2, u'GoldenEye (1995)'), (3, u'Four Rooms (1995)'), (4, u'Get Shorty (1995)'), (5, u'Copycat (1995)')]\n"
     ]
    }
   ],
   "source": [
    "def extractIdTitleFromItem(x):\n",
    "    tokens = x.split(\"|\")\n",
    "    return (int(tokens[0]), tokens[1])\n",
    "\n",
    "items = sc.textFile(\"data/meetup/movielens/u.item\", 5) \\\n",
    "            .map(extractIdTitleFromItem)             \n",
    "        \n",
    "print items.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50. Star Wars (1977) -> 583\n",
      "258. Contact (1997) -> 509\n",
      "100. Fargo (1996) -> 508\n",
      "181. Return of the Jedi (1983) -> 507\n",
      "294. Liar Liar (1997) -> 485\n",
      "286. English Patient, The (1996) -> 481\n",
      "288. Scream (1996) -> 478\n",
      "1. Toy Story (1995) -> 452\n",
      "300. Air Force One (1997) -> 431\n",
      "121. Independence Day (ID4) (1996) -> 429\n"
     ]
    }
   ],
   "source": [
    "# Join data and items \n",
    "ranked = rdd2.reduceByKey(lambda x, y: x + y)\\\n",
    "        .join(items)\\\n",
    "        .takeOrdered(10, key=lambda x: (-1 * x[1][0], x[1][1]))\n",
    "for row in ranked:\n",
    "    print \"{0}. {1} -> {2}\".format(row[0], row[1][1], row[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cleanup \n",
    "\n",
    "Currently the code is very frazile and we have to make sure that we are using correct indexes at each position. Let's use SQLContext to define tables and then ue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import SQLContext and data types\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# sc is an existing SparkContext.\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Construct Rating Tables by Attaching Schema to the loaded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct Rating Schema\n",
    "schema = StructType([\n",
    "                StructField(\"user\", IntegerType(), True),\n",
    "                StructField(\"movie\", IntegerType(), True),\n",
    "                StructField(\"rating\", IntegerType(), True),\n",
    "                StructField(\"timestamp\", IntegerType(), True)\n",
    "            ])\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaRating = sqlContext.createDataFrame(rdd1, schema)\n",
    "\n",
    "# Register the DataFrame as a table.\n",
    "schemaRating.registerTempTable(\"rating\")\n",
    "type(schemaRating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Construct Item Schema and Register as Temporary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct Movie Schema\n",
    "schema = StructType([\n",
    "            StructField('movie', IntegerType(), True)\n",
    "            , StructField('title', StringType(), True)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaItem = sqlContext.createDataFrame(items, schema)\n",
    "\n",
    "# Register the DataFrame as a table.\n",
    "schemaItem.registerTempTable(\"item\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Use SQLContext to operate on temporary tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(movie=1, title=u'Toy Story (1995)', cnt=452)\n",
      "Row(movie=50, title=u'Star Wars (1977)', cnt=583)\n",
      "Row(movie=100, title=u'Fargo (1996)', cnt=508)\n",
      "Row(movie=121, title=u'Independence Day (ID4) (1996)', cnt=429)\n",
      "Row(movie=181, title=u'Return of the Jedi (1983)', cnt=507)\n",
      "Row(movie=258, title=u'Contact (1997)', cnt=509)\n",
      "Row(movie=286, title=u'English Patient, The (1996)', cnt=481)\n",
      "Row(movie=288, title=u'Scream (1996)', cnt=478)\n",
      "Row(movie=294, title=u'Liar Liar (1997)', cnt=485)\n",
      "Row(movie=300, title=u'Air Force One (1997)', cnt=431)\n"
     ]
    }
   ],
   "source": [
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = sqlContext.sql(\"\"\"\n",
    "    SELECT A.movie, B.title, A.cnt\n",
    "    FROM\n",
    "    (\n",
    "        SELECT movie, count(*) as cnt\n",
    "        FROM rating \n",
    "        GROUP BY movie\n",
    "        ORDER BY cnt DESC\n",
    "        LIMIT 10\n",
    "    ) A\n",
    "    JOIN item B\n",
    "    ON (A.movie = B.movie) \n",
    "\"\"\")\n",
    "for row in results.collect():\n",
    "    print row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative To SQL: Using DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DataFrame Operations: https://spark.apache.org/docs/1.3.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "from pyspark.sql.functions import desc\n",
    "rdd2 = schemaRating.groupBy('movie').count().orderBy(desc(\"count\")).limit(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
