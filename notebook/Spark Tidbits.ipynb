{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Many new features are introduced in Spark 1.5. In this session we look at few of these features namely writing custom udf, \"withColumn\", broadcast/map side join, etc. You will need to download spark-1.5 from [here](http://www.apache.org/dyn/closer.lua/spark/spark-1.5.1/spark-1.5.1-bin-hadoop2.6.tgz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download dataset (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p data/meetup/movielens\n",
    "cd data/meetup/movielens\n",
    "rm -rf *\n",
    "wget http://www.grouplens.org/system/files/ml-100k.zip\n",
    "\n",
    "unzip -j ml-100k.zip \"ml-100k/u.data\" \n",
    "unzip -j ml-100k.zip \"ml-100k/u.item\" \n",
    "unzip -j ml-100k.zip \"ml-100k/u.user\" \n",
    "unzip -j ml-100k.zip \"ml-100k/README\"\n",
    "\n",
    "ls -lh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def toIntTuple(x):\n",
    "    return tuple([int(y) for y in x])\n",
    "\n",
    "data = sc.textFile(\"data/meetup/movielens/u.data\").map(lambda x: toIntTuple(x.split(\"\\t\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Schema and convert to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "schema = StructType([\n",
    "            StructField(\"user\", IntegerType(), nullable=False),\n",
    "            StructField(\"movie\", IntegerType(), nullable=False),\n",
    "            StructField(\"rating\", IntegerType(), nullable=False),\n",
    "            StructField(\"timestamp\", IntegerType(), nullable=False)\n",
    "    ])\n",
    "\n",
    "df = sqlContext.createDataFrame(data, schema)\n",
    "print df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Rating to Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "# define function\n",
    "def ratingToBinary(rating):\n",
    "    if rating >= 3: return 1\n",
    "    else: return 0\n",
    "\n",
    "# register as udf\n",
    "udfBinaryRating=udf(ratingToBinary, IntegerType())\n",
    "df1 = df.withColumn(\"binaryRating1\", udfBinaryRating(\"rating\"))\n",
    "df1.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Using when to replicate case statements\n",
    "Checkout other functions over [here](https://spark.apache.org/docs/1.4.0/api/python/_modules/pyspark/sql/functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.withColumn(\"binaryRating2\", when(col(\"rating\") >= 3, 1).otherwise(0)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broadcast/MapSide join\n",
    "Since 1.5 but only if using Scala. \n",
    "Will be available in 1.6 of pyspark .. see over [here](# this is available in 1.6 -- see pull request over [here](https://github.com/Jianfeng-chs/spark/blob/5bf51b8f96c1a9f1addef5d7001123b865eda0db/python/pyspark/sql/functions.py) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user age gender job           zipcode\n",
      "1    24  M      technician    85711  \n",
      "2    53  F      other         94043  \n",
      "3    23  M      writer        32067  \n",
      "4    24  M      technician    43537  \n",
      "5    33  F      other         15213  \n",
      "6    42  M      executive     98101  \n",
      "7    57  M      administrator 91344  \n",
      "8    36  M      administrator 05201  \n",
      "9    29  M      student       01002  \n",
      "10   53  M      lawyer        90703  \n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"user\", IntegerType(), nullable=False),\n",
    "        StructField(\"age\", IntegerType(), nullable=False),\n",
    "        StructField(\"gender\", StringType(), nullable=False),\n",
    "        StructField(\"job\", StringType(), nullable=False),\n",
    "        StructField(\"zipcode\", StringType(), nullable=False)\n",
    "    ])\n",
    "\n",
    "users = (sc\n",
    "             .textFile(\"data/meetup/movielens/u.user\")\n",
    "             .map(lambda x: x.split(\"|\"))\n",
    "             .map(lambda x: (int(x[0]), int(x[1]), x[2], x[3], x[4]))\n",
    "         )\n",
    "\n",
    "dfUser = sqlContext.createDataFrame(users, schema)\n",
    "dfUser.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.dataframe import DataFrame\n",
    "def broadcast(df):\n",
    "    sc = SparkContext._active_spark_context\n",
    "    return DataFrame(sc._jvm.functions.broadcast(df._jdf), df.sql_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.sql.functionsbroadcast does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-2a1e9564da3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfUser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"user\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-01f93502b24f>\u001b[0m in \u001b[0;36mbroadcast\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/bin/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m             raise Py4JError('{0} does not exist in the JVM'.\n\u001b[1;32m--> 662\u001b[1;33m                     format(self._fqn + name))\n\u001b[0m\u001b[0;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JError\u001b[0m: org.apache.spark.sql.functionsbroadcast does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "df.join(broadcast(dfUser),\"user\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
